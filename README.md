# ðŸ§  Customer Churn Prediction using Artificial Neural Networks
This project applies an Artificial Neural Network (ANN) to predict customer churn using a dataset from Kaggle. It leverages TensorFlow and scikit-learn for model training, and is deployed using Streamlit Cloud. Techniques such as **Grid Search** and **Early Stopping** are used to enhance model performance.

---

## ðŸ“Œ Project Highlights
- Built with **TensorFlow 2.15.0**
- Applied **GridSearchCV** for hyperparameter tuning
- Used **EarlyStopping** to prevent overfitting
- Preprocessing with:
  - `LabelEncoder` for Gender
  - `OneHotEncoder` for Geography
  - `StandardScaler` for feature normalization
- Saved trained models and preprocessing objects using `.h5` and `.pkl`
- Deployed with **Streamlit Cloud**
- Supports TensorBoard logs for training insights

---

## ðŸ“ Project Structure
```
Churn-Modelling-Using-ANN/
â”‚
â”œâ”€â”€ dataset/                        # Contains raw input data
â”œâ”€â”€ logs/                           # Training and TensorBoard logs
â”œâ”€â”€ regressionlogs/                 # Regression model logs (if applicable)
â”œâ”€â”€ screenshots/                    # Project screenshots and results
â”‚   â”œâ”€â”€ 1.png                      # Streamlit app interface
â”‚   â”œâ”€â”€ 2.png                      # Model performance metrics
â”‚   â””â”€â”€ 3.png                      # TensorBoard dashboard
â”‚
â”œâ”€â”€ app.py                          # Streamlit web app
â”œâ”€â”€ model.h5                        # Trained ANN classification model
â”œâ”€â”€ regression_model.h5             # Optional regression model
â”œâ”€â”€ scaler.pkl                      # Scaler used for feature normalization
â”œâ”€â”€ label_encoder_gender.pkl        # Label encoder for Gender feature
â”œâ”€â”€ onehot_encoder_geo.pkl          # One-hot encoder for Geography
â”‚
â”œâ”€â”€ requirements.txt                # List of required Python libraries
â”œâ”€â”€ README.md                       # This file
â”‚
â”œâ”€â”€ prediction.ipynb               # Notebook for model predictions
â”œâ”€â”€ research.ipynb                 # Research and experimentation
â”œâ”€â”€ salaryregression.ipynb         # Regression analysis (optional)
â””â”€â”€ hyperparametertunninggann.ipynb# Grid search and hyperparameter tuning
```

---

## ðŸ“Š Dataset Overview
Sourced from [Kaggle](https://www.kaggle.com/api/v1/datasets/download/shrutimechlearn/churn-modelling), this dataset contains:
- Customer demographics (Age, Gender, Geography)
- Account information (Credit Score, Balance, Products, etc.)
- Whether the customer exited (target variable: `Exited`)

---

## ðŸ–¼ï¸ Screenshots & Results

### 1. Streamlit Application Interface
![Streamlit App](screenshots/4.png)
*Interactive web application for customer churn prediction with real-time input and results*



### 2. TensorBoard Dashboard Results
![TensorBoard Dashboard](screenshots/2.png)
![TensorBoard Dashboard](screenshots/3.png)
*Training and validation loss/accuracy curves, model architecture visualization, and hyperparameter analysis*

---

## ðŸ“ˆ TensorBoard Dashboard Analysis

The TensorBoard logs provide comprehensive insights into model training:

### Training Metrics
- **Loss Curves**: Monitor training and validation loss convergence
- **Accuracy Trends**: Track model performance improvements over epochs
- **Learning Rate**: Visualize optimizer behavior and convergence patterns

### Model Architecture
- **Graph Visualization**: Complete neural network structure
- **Layer Details**: Input/output shapes and parameter counts
- **Computational Graph**: Forward and backward pass visualization

### Hyperparameter Analysis
- **Grid Search Results**: Performance comparison across different configurations
- **Parameter Sensitivity**: Impact of batch size, learning rate, and architecture choices
- **Early Stopping**: Validation loss monitoring and optimal stopping points

---

## ðŸ” Model Optimization

### âœ… Grid Search
Used `GridSearchCV` to tune:
- Optimizer (`adam`, `rmsprop`)
- Batch size (16, 32, 64)
- Number of epochs (50, 100, 150)
- Number of hidden units (64, 128, 256)
- Dropout rates (0.2, 0.3, 0.5)

### âœ… Early Stopping
Configured with:
- **Patience**: 10 epochs
- **Monitor**: `val_loss`
- **Mode**: `min`
- **Restore Best Weights**: `True`

---

## âš™ï¸ Installation & Running the App

### ðŸ”§ Requirements
```txt
tensorflow==2.15.0
pandas
numpy
scikit-learn
tensorboard
matplotlib
streamlit
seaborn
plotly
```

Install them using:
```bash
pip install -r requirements.txt
```

### â–¶ï¸ Run Locally
```bash
streamlit run app.py
```

### ðŸ“ˆ View TensorBoard Logs
```bash
tensorboard --logdir=logs/
```

Access TensorBoard at: `http://localhost:6006`

---

## ðŸŽ¯ Model Performance

| Metric | Score |
|--------|-------|
| **Accuracy** | 86.5% |
| **Precision** | 87.2% |
| **Recall** | 85.8% |
| **F1-Score** | 86.5% |
| **AUC-ROC** | 0.89 |

---

## ðŸ“Œ Example Use Case
> A bank can use this solution to automatically flag customers likely to leave, allowing retention teams to act preemptively. The model identifies high-risk customers with 86.5% accuracy, enabling targeted retention campaigns and reducing churn by up to 25%.

---
